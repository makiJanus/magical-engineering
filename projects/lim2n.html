<!DOCTYPE html>
<html lang="en ">

<head>
  <meta charset="UTF-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <!-- Primary Meta Tags -->
  <title>Magical Enginering</title>
  <meta name="title" content="Magical Enginering" />
  <meta name="description"
    content="Exploración de cómo usar LLM para generar comportamientos de navegación usando DeepSeek R1 y Isaac Sim." />
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.4/css/all.css"
    integrity="sha384-DyZ88mC6Up2uqS4h/KRgHuoeGwBcD4Ng9SiP4dIRy0EXTlnuz47vAwmeGwVChigm" crossorigin="anonymous" />
  <link rel="stylesheet" href="/src/css/style.css" />
  <link rel="stylesheet" href="/src/css/utilities.css" />
  <link rel="stylesheet" href="/src/css/blog_posts.css" />
  <link rel="stylesheet" href="/src/css/dialogue.css" />
</head>

<body>
  <header id="hero">
    <!-- Navbar -->
    <div id="navbar-placeholder"></div>

    <div id="content-blog">
      <h1 class="massiveHeading">Robots That Understand and Follow Human Instructions</h1>
      <p class="categories">
        <span class="category">Investigación</span>
        <span class="category">AI</span>
        <span class="category">Robótica</span>
      </p>

      <h2 class="subtitle">
        An experiment where I used LLMs to reason over semantic maps, generate paths, and control a Jetbot robot using PD. The key? A good prompt.
      </h2>

      <h3 class="under-subtitle-link"><a href="https://discovery.ucl.ac.uk/id/eprint/10206799/">Read the article in which this porject is inspired here!</a></h3>

      <p></p>
      <h3>The Problem</h3>
      <p>Wouldn't it be amazing to just tell a robot, "go get that" and have it actually do it, just like in the movies? Well, thanks to large language models (LLMs), that's now a real possibility. But how can a model that only processes text actually control a robot in the physical world?</p>

      <img src="/src/project_posts/project10/intro_img.png" alt="Isaac Sim robots and environments">

      <h3>Where It All Begins – The Source of Inspiration</h3>
      <p>Weiqin Zu and their team had the same question. Their proposal? A system called:

"Language and Sketching: An LLM-driven Interactive Multimodal Multitask Robot Navigation Framework"

A long title that basically means: you can give orders to a robot using text and little drawings.</p>

      <img src="/src/project_posts/project10/intro_1.png" alt="Isaac Sim research diagram">

      <p>Even though the authors didn’t release their code, they did share a very helpful diagram explaining how the system works:</p>

      <img src="/src/project_posts/project10/intro_2.png" alt="Isaac Sim research diagram">

      <h3>System Overview</h3>
      <p>The LIM2N system is divided into three main modules that work together to turn human instructions into robotic movement:</p>

<ol>

  <p><strong>LLM Module (Language)</strong>: This is the part of the system that reasons using language. Its job is to understand what the user wants the robot to do. This module converts natural language into structured data: task type, final destination, and environmental constraints.</p>
  <ul>
    <li><strong>Instruction:</strong> Everything starts with a command, which can be spoken or written, such as “Take the VIP to the fridge, but go past the shelf.”</li>
    <li><strong>Semantic Map:</strong> Since the LLM cannot directly see the environment, it is given a semantic map that describes what objects are present, where they are, and how they relate to each other.</li>
    <li><strong>Function Library:</strong> These are small mathematical and logical tools that the LLM can use to interpret instructions. For example, it can calculate what “in front of the fridge” means in real coordinates.</li>
  </ul>

  <p></p>

  <p><strong>Intelligent Sensing Module</strong>: This is the system’s “eye.” It merges what the user draws with what the robot detects through its sensors. Additionally, if the task involves people, this module also detects and tracks their movements with high precision.</p>
  <ul>
    <li>Users can mark preferred routes, danger zones, or places to avoid through an interface.</li>
    <li>The system combines these drawings with real-time data from the lidar and camera.</li>
    <li>The result is an <strong>enriched map</strong> that the robot can use to navigate more safely.</li>
  </ul>

  <p></p>

  <p><strong>Reinforcement Learning Module (RL)</strong>: This is the decision-making engine. It is responsible for turning the previously gathered information into concrete actions. This module ensures that the robot not only knows <em>what</em> to do, but also <em>how</em> to do it well, even in new situations.</p>
  <ul>
    <li><strong>Task Processor:</strong> This component organizes all the information and tells the robot: “this is your mission, these are the obstacles, and that is the destination.”</li>
    <li><strong>SAC (Soft Actor-Critic):</strong> This is the algorithm that controls movement. It continuously learns how to move better, avoid obstacles, and adjust its behavior based on the environment.</li>
  </ul>
</ol>

<h3>Simplifying and replicating</h3>
<p>My main interest is in semantic reasoning: how a robot goes from “avoid the carpet and go past the shelf” to a sequence of coordinates.</p>
<p>That’s why I decided to replicate the LLM module inside a Docker container accessible via API. Instead of the original RL system, I use a generic navigation module with A* and PD control in Isaac Sim (which can be replaced later with more advanced planners).</p>

      <img src="/src/project_posts/project10/sr_1.png" alt="Isaac Sim research diagram">

      <p>This is what the user interface looks like in Isaac Sim:</p>

      <img src="/src/project_posts/project10/sr_2.png" alt="Isaac Sim research diagram">
      <img src="/src/project_posts/project10/sr_3.png" alt="Isaac Sim research diagram">

      <p>And this is how the robot looks while following a path generated with LLM + A* using PD control:</p>

      <img src="/src/project_posts/project10/sr_4.gif" alt="Isaac Sim research diagram">

      <h3>The Prompt That Does the Magic</h3>
      <p>A key part of the system is the prompt. It explains everything the LLM needs to know about the map, the user’s instruction, and the expected output up to 5 waypoints.</p>

      <div class="code-container scrollable-code-container">

          <pre><code>
    def reason_goals(self, user_prompt: str) -> str:
        """
        Send a system prompt followed by a user query to the LLM and return the full response.
        """
        
        system_prompt = """
            The user is to provide set of instructrions considering a map, the semantic representation of the objects in it, and considerations to generate points that in a later stage a robot must follow.
            Your sole purpose is to analyze semantically the provided map in array format and, from the user considerations, use leters  (a,b,c,d, ...) to mark the different goal points that (in a later stage) a robot should go to move as the user wants it to move.

            CONSIDERATIONS:
            - You will be provide with an array and the semantic meaning of its elemets, consider them carefully to think were the goal points can be placed.
            - Understand the spatial relation between objects to select the best positions possible for each goal, if some consideration of the user is not possible, select the nearest point that make sense.
            - THE FIRST POINT IS a, THEN b, c, d, e, ... AND SO ON, CREATE THE GOALS IN ORDER, THAT IS THE SAME FROM CLOSEST TO FARTHEST.
            - From the arragement of the elements on the map, deduce the orientation of each one, whic part is the front or behind it, left and right, the use these insights as context for the goal points generation.
            - Oonly select up to 5 points (a,b,c,d,e) on the most relevant parts accordingly to the user instruction.
            - DO NOT USE COORDINATES RELATED TO OBBJECTS OF THE MAP AS GOAL POINTS, CHOOSE ONLY THE FREE SPACE THAT MAKES SENSE FOR THE ROBOT TO MOVE. 
            - CONSIDER SELECTING GOAL POINTS THAT ARE NOT RIGHT NEXT TO THE OBJECTS, BUT IN THE NEARBY AREA, SO THE ROBOT CAN MOVE SAFELY, IF POSSIBLE.
            - DO NOT, IF POSSIBLE, SELECT GOAL POINTS RIGHT NEXT TO THE WALL, ALSO, IF YOU SELECT TWO GOAL POINTS AND IN BETWEEN THERE IS AN OBJECT, CONSIDER CREATING ANOTHER POINT NEAR THE OBJECT SO THE ROBO CAN KNOW HOW TO CIRCUMVENT THE OBJECT.
            - There will may be a buffer zone around the objects, consider it as a zone to avoid, but not as a wall.
            - USE REASONING AND CHAIN OF THOUGHT TO UNDERSTAND AND SOLVE THE PROBLEM.

            TO DELIVER:
            - A set of points that the robot must follow in that order to accomplish the user instruction, UP TO 5 POINTS:
            a = (15, 7)
            b = (16, 13)
            c = (20, 20)
            d = (22, 25)

            Here there is an example:

            Pass between the sofa and tv, then go to the carpet

            -1 - robot initial position
            -2 - buffer zone, to avoid as much as possible
            0  - free space
            1  - wall
            2  - TV
            3  - sofa
            4  - carpet
            5  - box

            [[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]
            [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
            [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
            [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
            [1. 0. 0. 0. -1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
            [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
            [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
            [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
            [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
            [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
            [1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
            [1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 4. 4. 4. 4. 4. 0. 1.]
            [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 0. 4. 4. 4. 4. 4. 0. 1.]
            [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 4. 4. 4. 4. 4. 0. 1.]
            [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 4. 4. 4. 4. 4. 0. 1.]
            [1. 0. 0. 0. 0. 0. 0. a  0. 0. 0. 0. 0. b  0. 0. 0. 0. 0. 0. c  0. 0. 4. 4. d  4. 4. 0. 1.]
            [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 4. 4. 4. 4. 4. 0. 1.]
            [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 4. 4. 4. 4. 4. 0. 1.]
            [1. 0. 0. 0. 0. 0. 0. 0. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 0. 0. 0. 4. 4. 4. 4. 4. 0. 1.]
            [1. 0. 0. 0. 0. 0. 0. 0. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 0. 0. 0. 4. 4. 4. 4. 4. 0. 1.]
            [1. 0. 0. 0. 0. 0. 0. 0. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 0. 0. 0. 4. 4. 4. 4. 4. 0. 1.]
            [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
            [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
            [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
            [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
            [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
            [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 5. 5. 5. 5. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
            [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 5. 5. 5. 5. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
            [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 5. 5. 5. 5. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
            [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 5. 5. 5. 5. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
            [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]
            
            Semantic Understanding:
            - TV (2) is located around row 12, columns 10–17.
            - Sofa (3) is in rows 19–21, columns 8–19.
            - There's a passage area around row 15–17 and columns 8–19 between them.
            - Carpet (4) occupies rows 11–21, columns 23–27.
            - Robot starts at (4, 4).

            Useful insights
            [generate them]

            Goal Point Logic:
            a: Go toward the center of the passage between the sofa and TV (aligned horizontally).
            b: Recenter after passing the middle of the sofa-TV area.
            c: Enter the carpet area from the left side.
            d: Move toward the center of the carpet.

            So, the goal points (a, b, c, d) are:
            a = (15, 7) → Approaching passage between TV and sofa
            b = (15, 13) → Between TV and sofa
            c = (15, 20) → Leaving passage, entering carpet direction
            d = (15, 25) → On the carpet, central area

            Answer:
            a = (15, 7)
            b = (15, 13)
            c = (15, 20)
            d = (15, 25)

            Now waiting for the next user instruction.
            """
        
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]

        payload = {
            "model": self.model,
            "messages": messages
        }

        response = requests.post(
            url=self.base_url,
            headers=self.headers,
            data=json.dumps(payload)
        )

        response.raise_for_status()  # Raise an error for bad responses

        response_data = response.json()
        return response_data["choices"][0]["message"]["content"]
"""
    </code></pre>

    </div>

    <p></p>

  <p>This prompt is long and detailed, but its structure is clear:</p>
  <ol>
    <li>System role.</li>
    <li>Spatial considerations.</li>
    <li>Format of the expected result.</li>
    <li>Step-by-step reasoned example.</li>
    <li>Final result.</li>
  </ol>

  <p>This allows the model to use "chain-of-thought" to generate a robust and reliable response.</p>

  <h3>Experimental Results</h3>
  <p>I used the same map and instructions as the original paper, but with a different mobile base: the Jetbot, which is non-holonomic (only two active wheels, final pose depends on the initial orientation). The paper uses a holonomic robot with Mecanum wheels.</p>

    <img src="/src/project_posts/project10/re_1.png" alt="Isaac Sim research outcome paths">
    <img src="/src/project_posts/project10/re_2.png" alt="Isaac Sim research outcome paths">
    <img src="/src/project_posts/project10/re_3.png" alt="Isaac Sim research outcome paths">

  <p>The paper compares three methods: manual control, unconstrained RL, and the LLM+RL system. The latter achieves a higher overall success rate.</p>

  <img src="/src/project_posts/project10/re_4.png" alt="Isaac Sim research outcome paths">

  <p>In my case, I compared the reference path generated by A* with the robot’s actual path using PD control:</p>

  <img src="/src/project_posts/project10/re_5.png" alt="Isaac Sim research outcome paths">

  <p>The results show that the robot follows the path well. The small deviations are due to its non-holonomic base. In tests 1 through 4, the instructions are correctly followed. In tests 5 and 6, the LLM interprets the commands very literally (for example, going to the last mentioned object). This indicates that the prompt could be improved for better contextual understanding.</p>

  <p>🎉 Thanks for reading! Feel free to contact me if you have any questions.</p>

    </div>
    <div class="division-space-only"></div>

    <!-- Footer -->
    <div id="footer-placeholder"></div>

    <script src="/src/js/navbar_footer.js"></script>
</body>

</html>